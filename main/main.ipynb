{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7b7c350ff0352",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Creating Tokens from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b528b478b4bb9fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:09:01.702236Z",
     "start_time": "2024-10-12T01:09:01.697491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the text file:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"../ch02/01_main-chapter-code/the-verdict.txt\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of characters in the text file: \", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd69bb6fb8f1a6",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718197393b9976d",
   "metadata": {},
   "source": [
    "### Getting Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7844cb0370514f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:10:59.873350Z",
     "start_time": "2024-10-12T01:10:59.860627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
      "4690\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "result = [item for item in result if item.strip()] # Remove space strings\n",
    "print(result[:30])\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23572d8d3a375e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:11:04.469200Z",
     "start_time": "2024-10-12T01:11:04.464507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in the text file:  1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(result))  # Get unique words and sort them\n",
    "vocab_size = len(all_words)\n",
    "print(\"Total number of unique words in the text file: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7fe89a966d7dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:11:19.497051Z",
     "start_time": "2024-10-12T01:11:19.493929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('yet', 1125)\n",
      "('you', 1126)\n",
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_words)}  # Create a dictionary of tokens and their IDs\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e68ba55b5df7fb",
   "metadata": {},
   "source": [
    "### Implementing a Simple Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7211cdf4264ddbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:12:41.624109Z",
     "start_time": "2024-10-12T01:12:41.620924Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocabulary): # We must have a vocabulary to initialize the tokenizer\n",
    "        self.str_to_int = vocabulary  # Dictionary of tokens and their IDs {\"token1\": 0, ...}\n",
    "        self.int_to_str = {v: k for k, v in vocabulary.items()}  # Dictionary of IDs and their tokens {0: \"token1\", ...}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert text to a list of token IDs\n",
    "        preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed_text = [item.strip() for item in preprocessed_text if item.strip()]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed_text]\n",
    "        return ids  # return a list of token IDs\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Convert a list of token IDs to text with spaces\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)  # Remove spaces before punctuation\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a1b15a39b1ffa2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:12:47.162129Z",
     "start_time": "2024-10-12T01:12:47.159282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace8d81aa5602c1",
   "metadata": {},
   "source": [
    "### Implementing a Simple Text Tokenizer that Handles Unknown Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c971b07cb4c48dbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:13:47.412361Z",
     "start_time": "2024-10-12T01:13:47.408958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])  # Add special tokens for end of text and unknown words\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}  # Create a new vocabulary\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e145af44ad6aeb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:14:20.871491Z",
     "start_time": "2024-10-12T01:14:20.868445Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocabulary): # We must have a vocabulary to initialize the tokenizer\n",
    "        self.str_to_int = vocabulary  # Dictionary of tokens and their IDs {\"token1\": 0, ...}\n",
    "        self.int_to_str = {v: k for k, v in vocabulary.items()}  # Dictionary of IDs and their tokens {0: \"token1\", ...}\n",
    "\n",
    "    def encode(self, text) -> list:\n",
    "        preprocessed_tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed_tokens = [item.strip() for item in preprocessed_tokens if item.strip()]  # remove empty space strings and empty strings\n",
    "        preprocessed_tokens = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed_tokens]  # Replace unknown words from source text with <|unk|>\n",
    "\n",
    "        ids = [self.str_to_int[token] for token in preprocessed_tokens]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids) -> str:\n",
    "        decoded_text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        decoded_text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', decoded_text) # Remove spaces before punctuation\n",
    "        return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c4b2eb6219cc0be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:14:47.070581Z",
     "start_time": "2024-10-12T01:14:47.067087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3702a504b259f",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding (BPE): A Tokenzier Which can Handle Any Unknown Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a0f5cce7abbba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:15:50.200549Z",
     "start_time": "2024-10-12T01:15:50.053134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "# Use tiktoken library to implement BPE\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "print(version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3159c0145b671788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:15:59.530233Z",
     "start_time": "2024-10-12T01:15:57.757788Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67eb3d3559e431b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:16:08.762270Z",
     "start_time": "2024-10-12T01:16:08.759111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13, 9084, 86, 343, 86, 220, 959, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace. Akwirw ier.\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace. Akwirw ier.\"\n",
    "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(ids)\n",
    "\n",
    "text = tokenizer.decode(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9fce13d5fcff7",
   "metadata": {},
   "source": [
    "# Creating Input and Targets Data Loader Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737079ea4834e95",
   "metadata": {},
   "source": [
    "## Data Sampling with A Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bd62bc921ebd3cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:17:45.133491Z",
     "start_time": "2024-10-12T01:17:45.127679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# Encode the text using the BPE tokenizer\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8750edf9b4471c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:21:34.595289Z",
     "start_time": "2024-10-12T01:21:34.590114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      "\n",
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      "\n",
      "\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "# Take a sample of encoded text\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "# Create Input/Target Pairs by using a sliding window\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size] # Input is the first 4 tokens of sampled text [0:4]\n",
    "y = enc_sample[1:context_size+1] # Target is the next 4 tokens of sampled text [1:5] (Right shift by 1)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\\n\")\n",
    "\n",
    "# context and desired are encoded token ids (integers)\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Do the above loop again with the decoded token ids\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]  # list\n",
    "    desired = enc_sample[i]  # integer, so need to convert to list\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825935b3b12a4b7",
   "metadata": {},
   "source": "## Using a PyTorch Data Loader To Create Input and Target Tensors"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ba25d459e00a27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:23:43.857517Z",
     "start_time": "2024-10-12T01:23:43.855229Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8764854d9e99eed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:24:06.692495Z",
     "start_time": "2024-10-12T01:24:06.688235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Dataset class\n",
    "class GPTDataSetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \"\"\"\n",
    "        :param txt: source text\n",
    "        :param tokenizer: tokenizer object\n",
    "        :param max_length: number of token ids in a chunk\n",
    "        :param stride: shift across batches. For example, input/target pair batch 1 is [0:4]/[1:5], batch 2 is [1:5]/[2:6], etc.\n",
    "        \"\"\"\n",
    "        self.input_ids = []  # A list of tensors, or a multi-dimension array\n",
    "        self.target_ids = [] # A list of tensors, or a multi-dimension array\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)  # Tokenize the text and create token ids\n",
    "\n",
    "        # Create input/target pairs using a sliding window\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx) -> (Tensor, Tensor):\n",
    "        # Returns a pair of 1D tensors (input_ids, target_ids) at the index idx\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae9cd9f731cc7648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:25:52.888123Z",
     "start_time": "2024-10-12T01:25:52.883383Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataSetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c7ddb305f72082c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:26:14.987451Z",
     "start_time": "2024-10-12T01:26:14.966936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464],\n",
      "        [1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899]])]\n"
     ]
    }
   ],
   "source": [
    "# batch_size: number of batches in input tensors or target tensors\n",
    "# max_length: number of token ids in a batch\n",
    "# stride: shift across batches in input tensors\n",
    "data_loader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(data_loader) # Iteration of the input/target tensor pairs\n",
    "print(next(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d524dc645919e",
   "metadata": {},
   "source": [
    "# Creating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57493189dc8fe1",
   "metadata": {},
   "source": [
    "## Input Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a65c35fbc7bc0b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:33:25.243132Z",
     "start_time": "2024-10-12T01:33:25.069905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Token IDs (per batch): \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "\n",
      "Embedding Layer:\n",
      " Embedding(50257, 256)\n",
      "\n",
      "Input Embeddings Shape:\n",
      " torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Creating a Dataloader\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=8, \n",
    "    max_length=max_length, \n",
    "    stride=max_length, \n",
    "    shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Input Token IDs (per batch): \\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "# Create an embedding layer\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(\"\\nEmbedding Layer:\\n\", token_embedding_layer)\n",
    "input_embeddings = token_embedding_layer(inputs)\n",
    "print(\"\\nInput Embeddings Shape:\\n\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c3d426cd5dcd8",
   "metadata": {},
   "source": [
    "## Input Positional Embeddings (Context Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb6213fd3d3dcfc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:34:16.800039Z",
     "start_time": "2024-10-12T01:34:16.796752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length # Positional context has the same number of input tokens in a batch\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)  # We only need the same number of positional embeddings as the number of tokens in a batch\n",
    "\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))  # torch.arrange creates a tensor with values from 0 to context_length-1\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0bcf88a54ea65",
   "metadata": {},
   "source": [
    "## Input Embeddings: Combine Input Token Embeddings and Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eac408d75404cb4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:34:44.663914Z",
     "start_time": "2024-10-12T01:34:44.661449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = input_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Self-Attention Mechanism",
   "id": "e9666d4f0c881397"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T01:46:50.491104Z",
     "start_time": "2024-10-12T01:46:50.484803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Say, we have a text input sequence of 6 word and each word/token has 3 dimension embedding.\n",
    "# So, the input sequence is of shape (6, 3)\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # word 1, Your (x^1)\n",
    "        [0.55, 0.87, 0.66], # word 2, journey (x^2)\n",
    "        [0.57, 0.85, 0.64], # word 3, starts (x^3)\n",
    "        [0.22, 0.58, 0.33], # word 4, with (x^4)\n",
    "        [0.77, 0.25, 0.10], # word 5, one (x^5)\n",
    "        [0.05, 0.80, 0.55], # word 6, step (x^6)\n",
    "    ]\n",
    ")"
   ],
   "id": "79139e2d0dfd4774",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simplified Self-Attention",
   "id": "7ca13e84cc0a6646"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Attention Scores",
   "id": "9e744b04147c176e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T04:52:02.180394Z",
     "start_time": "2024-10-12T04:52:02.160112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a matrix of Attention Scores\n",
    "attn_scores = torch.empty(inputs.shape[0], inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)  # dot product of each word/token with each other\n",
    "\n",
    "print(\"Attention scores matrix:\\n\", attn_scores)\n",
    "\n",
    "# Or, Calculate the Attention Score matrix by using matrix multiplication\n",
    "attn_scores = torch.mm(inputs, torch.t(inputs)) # Input matrix x Input matrix transpose\n",
    "\n",
    "print(\"Attention scores matrix:\\n\", attn_scores)\n",
    "\n",
    "# Or, a simplified presentation of matrix multiplication\n",
    "attn_scores = inputs @ inputs.T\n",
    "\n",
    "print(\"Attention scores matrix:\\n\", attn_scores)"
   ],
   "id": "c556d21dda3a88b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores matrix:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "Attention scores matrix:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "Attention scores matrix:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Attention Weights",
   "id": "1e32cd89232a04ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T04:52:14.119964Z",
     "start_time": "2024-10-12T04:52:14.113719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(\"Attention weights:\\n\", attn_weights)"
   ],
   "id": "edd03877aa678671",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      " tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Context Embeddings",
   "id": "36fb8a00f9dbb737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T04:53:32.256204Z",
     "start_time": "2024-10-12T04:53:32.252703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "## Calculate the Context Embedding\n",
    "context_embeddings = attn_weights @ inputs\n",
    "print(\"Context embeddings:\\n\", context_embeddings)"
   ],
   "id": "6d105320d85cedb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context embeddings:\n",
      " tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scaled Dot-Product Attention",
   "id": "6203cc5a8816a0ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Query, Key, and Value Matrices",
   "id": "458fa13be68103c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T00:02:14.890914Z",
     "start_time": "2024-10-13T00:02:14.883865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "d_in = inputs.shape[1]   # This is dimension of the input token, M in the NxM\n",
    "d_out = inputs.shape[1]\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ],
   "id": "3bbeeab6b0a2ab2",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T00:02:16.124310Z",
     "start_time": "2024-10-13T00:02:16.114339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = inputs @ W_query\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"Query:\", query)\n",
    "print(\"Key:\", keys)\n",
    "print(\"Value:\", values)"
   ],
   "id": "ef7720f2e716fe37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: tensor([[0.3522, 0.3244, 0.4020],\n",
      "        [0.8520, 0.4161, 1.0138],\n",
      "        [0.8415, 0.4229, 0.9978],\n",
      "        [0.5096, 0.1904, 0.6187],\n",
      "        [0.4138, 0.4265, 0.4288],\n",
      "        [0.6408, 0.1414, 0.8070]])\n",
      "Key: tensor([[0.6813, 0.2706, 1.0793],\n",
      "        [0.7305, 0.4227, 1.1993],\n",
      "        [0.7355, 0.4227, 1.1901],\n",
      "        [0.3363, 0.2225, 0.6077],\n",
      "        [0.6184, 0.3038, 0.6909],\n",
      "        [0.3178, 0.2383, 0.7426]])\n",
      "Value: tensor([[0.4976, 0.9655, 0.7614],\n",
      "        [0.9074, 1.3518, 1.5075],\n",
      "        [0.8976, 1.3391, 1.4994],\n",
      "        [0.5187, 0.7319, 0.8493],\n",
      "        [0.4699, 0.7336, 0.9307],\n",
      "        [0.6446, 0.9045, 0.9814]])\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Attention Scores",
   "id": "661d583cf291800f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T00:02:28.625363Z",
     "start_time": "2024-10-13T00:02:28.621533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_scores = query @ keys.T\n",
    "print(\"Attention Score: \", attn_scores)"
   ],
   "id": "3e56d39eb2ef8272",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Score:  tensor([[0.7616, 0.8765, 0.8746, 0.4349, 0.5941, 0.4877],\n",
      "        [1.7872, 2.0141, 2.0091, 0.9952, 1.3538, 1.1227],\n",
      "        [1.7646, 1.9901, 1.9852, 0.9834, 1.3383, 1.1091],\n",
      "        [1.0664, 1.1947, 1.1916, 0.5897, 0.8004, 0.6667],\n",
      "        [0.8601, 0.9968, 0.9950, 0.4947, 0.6817, 0.5516],\n",
      "        [1.3458, 1.4957, 1.4915, 0.7374, 0.9968, 0.8366]])\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Attention Weights",
   "id": "d86d5870ad49b94c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T00:03:25.517589Z",
     "start_time": "2024-10-13T00:03:25.512391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "print(\"Attention weights:\", attn_weights)"
   ],
   "id": "4a1c829d441e7f26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([[0.1747, 0.1866, 0.1864, 0.1446, 0.1586, 0.1491],\n",
      "        [0.1862, 0.2123, 0.2117, 0.1179, 0.1450, 0.1269],\n",
      "        [0.1859, 0.2118, 0.2112, 0.1184, 0.1454, 0.1273],\n",
      "        [0.1798, 0.1936, 0.1932, 0.1365, 0.1542, 0.1427],\n",
      "        [0.1751, 0.1895, 0.1893, 0.1418, 0.1579, 0.1465],\n",
      "        [0.1837, 0.2003, 0.1998, 0.1293, 0.1501, 0.1369]])\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Context Embeddings",
   "id": "efecafb33982f6ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T00:04:38.162487Z",
     "start_time": "2024-10-13T00:04:38.158136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_embeddings = attn_weights @ values\n",
    "print(\"Context embedding:\", context_embeddings)"
   ],
   "id": "f5e902501d2fccc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context embedding: tensor([[0.6692, 1.0276, 1.1106],\n",
      "        [0.6864, 1.0577, 1.1389],\n",
      "        [0.6860, 1.0570, 1.1383],\n",
      "        [0.6738, 1.0361, 1.1180],\n",
      "        [0.6711, 1.0307, 1.1139],\n",
      "        [0.6783, 1.0441, 1.1252]])\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Implementing a Scaled Dot-Product Attention Class",
   "id": "1b584f558bfdf89b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T00:05:27.693107Z",
     "start_time": "2024-10-13T00:05:27.688139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use nn.Parameter to create weight matrices\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(SelfAttention_v1, self).__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        query = inputs @ self.W_query\n",
    "        key = inputs @ self.W_key\n",
    "        value = inputs @ self.W_value\n",
    "\n",
    "        attn_scores = query @ key.T\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_embedding = attn_weights @ value\n",
    "        return context_embedding"
   ],
   "id": "ce8fc8f0864889be",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T00:05:47.880130Z",
     "start_time": "2024-10-13T00:05:47.873591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the SelfAttention_v1 class\n",
    "torch.manual_seed(123)\n",
    "\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "self_attn_v1 = SelfAttention_v1(d_in, d_out)\n",
    "context_embedding_v1 = self_attn_v1(inputs)\n",
    "print(\"Context embeddings:\\n\", context_embedding_v1)"
   ],
   "id": "7fd2975eb341ff32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context embeddings:\n",
      " tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T00:06:01.238119Z",
     "start_time": "2024-10-13T00:06:01.235154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use nn.Linear to create weight matrices\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.attn_weights = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        query = self.W_query(inputs)\n",
    "        key = self.W_key(inputs)\n",
    "        value = self.W_value(inputs)\n",
    "\n",
    "        attn_scores = query @ key.T\n",
    "        self.attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_embedding = attn_weights @ value\n",
    "        return context_embedding"
   ],
   "id": "80318d005f6274e0",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T00:06:08.046493Z",
     "start_time": "2024-10-13T00:06:08.036933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the SelfAttention_v2 class\n",
    "torch.manual_seed(789)\n",
    "\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "self_attn_v2 = SelfAttention_v2(d_in, d_out)\n",
    "context_embedding_v2 = self_attn_v2(inputs)\n",
    "print(\"Context embeddings:\\n\", context_embedding_v2)"
   ],
   "id": "c5586fbf6fb4d217",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context embeddings:\n",
      " tensor([[-0.0776,  0.0699],\n",
      "        [-0.0789,  0.0732],\n",
      "        [-0.0789,  0.0732],\n",
      "        [-0.0781,  0.0707],\n",
      "        [-0.0775,  0.0706],\n",
      "        [-0.0785,  0.0714]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Causal Attention",
   "id": "882aaf7abc8715d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "903eda00fe9c8eb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
