{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7b7c350ff0352",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Creating Tokens from Text"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b528b478b4bb9fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:33.839312Z",
     "start_time": "2024-10-24T16:26:33.833452Z"
    }
   },
   "source": [
    "with open(\"../ch02/01_main-chapter-code/the-verdict.txt\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of characters in the text file: \", len(raw_text))\n",
    "print(raw_text[:99])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the text file:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "b2fd69bb6fb8f1a6",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718197393b9976d",
   "metadata": {},
   "source": [
    "### Getting Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "id": "5d7844cb0370514f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:33.845326Z",
     "start_time": "2024-10-24T16:26:33.842126Z"
    }
   },
   "source": [
    "import re\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "result = [item for item in result if item.strip()] # Remove space strings\n",
    "print(result[:30])\n",
    "print(len(result))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
      "4690\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "23572d8d3a375e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:33.863553Z",
     "start_time": "2024-10-24T16:26:33.861239Z"
    }
   },
   "source": [
    "all_words = sorted(set(result))  # Get unique words and sort them\n",
    "vocab_size = len(all_words)\n",
    "print(\"Total number of unique words in the text file: \", vocab_size)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in the text file:  1130\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "2f7fe89a966d7dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:33.880345Z",
     "start_time": "2024-10-24T16:26:33.877966Z"
    }
   },
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_words)}  # Create a dictionary of tokens and their IDs\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n",
    "    if i > 10:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('yet', 1125)\n",
      "('you', 1126)\n",
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "31e68ba55b5df7fb",
   "metadata": {},
   "source": [
    "### Implementing a Simple Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7211cdf4264ddbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:33.921298Z",
     "start_time": "2024-10-24T16:26:33.918136Z"
    }
   },
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocabulary): # We must have a vocabulary to initialize the tokenizer\n",
    "        self.str_to_int = vocabulary  # Dictionary of tokens and their IDs {\"token1\": 0, ...}\n",
    "        self.int_to_str = {v: k for k, v in vocabulary.items()}  # Dictionary of IDs and their tokens {0: \"token1\", ...}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert text to a list of token IDs\n",
    "        preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed_text = [item.strip() for item in preprocessed_text if item.strip()]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed_text]\n",
    "        return ids  # return a list of token IDs\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Convert a list of token IDs to text with spaces\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)  # Remove spaces before punctuation\n",
    "        return text"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "4a1b15a39b1ffa2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:34.009878Z",
     "start_time": "2024-10-24T16:26:34.007714Z"
    }
   },
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "dace8d81aa5602c1",
   "metadata": {},
   "source": [
    "### Implementing a Simple Text Tokenizer that Handles Unknown Tokens"
   ]
  },
  {
   "cell_type": "code",
   "id": "c971b07cb4c48dbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:34.041336Z",
     "start_time": "2024-10-24T16:26:34.039012Z"
    }
   },
   "source": [
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])  # Add special tokens for end of text and unknown words\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}  # Create a new vocabulary\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "2e145af44ad6aeb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:34.072241Z",
     "start_time": "2024-10-24T16:26:34.069462Z"
    }
   },
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocabulary): # We must have a vocabulary to initialize the tokenizer\n",
    "        self.str_to_int = vocabulary  # Dictionary of tokens and their IDs {\"token1\": 0, ...}\n",
    "        self.int_to_str = {v: k for k, v in vocabulary.items()}  # Dictionary of IDs and their tokens {0: \"token1\", ...}\n",
    "\n",
    "    def encode(self, text) -> list:\n",
    "        preprocessed_tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed_tokens = [item.strip() for item in preprocessed_tokens if item.strip()]  # remove empty space strings and empty strings\n",
    "        preprocessed_tokens = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed_tokens]  # Replace unknown words from source text with <|unk|>\n",
    "\n",
    "        ids = [self.str_to_int[token] for token in preprocessed_tokens]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids) -> str:\n",
    "        decoded_text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        decoded_text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', decoded_text) # Remove spaces before punctuation\n",
    "        return decoded_text"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "5c4b2eb6219cc0be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:34.121108Z",
     "start_time": "2024-10-24T16:26:34.118696Z"
    }
   },
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "43c3702a504b259f",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding (BPE): A Tokenzier Which can Handle Any Unknown Words"
   ]
  },
  {
   "cell_type": "code",
   "id": "27a0f5cce7abbba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:34.260465Z",
     "start_time": "2024-10-24T16:26:34.134521Z"
    }
   },
   "source": [
    "# Use tiktoken library to implement BPE\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "print(version(\"tiktoken\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.2\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "3159c0145b671788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:36.119583Z",
     "start_time": "2024-10-24T16:26:34.261339Z"
    }
   },
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "67eb3d3559e431b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:36.123041Z",
     "start_time": "2024-10-24T16:26:36.120516Z"
    }
   },
   "source": [
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace. Akwirw ier.\"\n",
    "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(ids)\n",
    "\n",
    "text = tokenizer.decode(ids)\n",
    "print(text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13, 9084, 86, 343, 86, 220, 959, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace. Akwirw ier.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "eda9fce13d5fcff7",
   "metadata": {},
   "source": [
    "# Creating Input and Targets Data Loader Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737079ea4834e95",
   "metadata": {},
   "source": [
    "## Data Sampling with A Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "id": "5bd62bc921ebd3cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:36.128452Z",
     "start_time": "2024-10-24T16:26:36.124214Z"
    }
   },
   "source": [
    "# Encode the text using the BPE tokenizer\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "e8750edf9b4471c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:36.132150Z",
     "start_time": "2024-10-24T16:26:36.129073Z"
    }
   },
   "source": [
    "# Take a sample of encoded text\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "# Create Input/Target Pairs by using a sliding window\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size] # Input is the first 4 tokens of sampled text [0:4]\n",
    "y = enc_sample[1:context_size+1] # Target is the next 4 tokens of sampled text [1:5] (Right shift by 1)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\\n\")\n",
    "\n",
    "# context and desired are encoded token ids (integers)\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Do the above loop again with the decoded token ids\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]  # list\n",
    "    desired = enc_sample[i]  # integer, so need to convert to list\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      "\n",
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      "\n",
      "\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "b825935b3b12a4b7",
   "metadata": {},
   "source": "## Using a PyTorch Data Loader To Create Input and Target Tensors"
  },
  {
   "cell_type": "code",
   "id": "7ba25d459e00a27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:38.891728Z",
     "start_time": "2024-10-24T16:26:36.132796Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "8764854d9e99eed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:38.895269Z",
     "start_time": "2024-10-24T16:26:38.892446Z"
    }
   },
   "source": [
    "# Define the Dataset class\n",
    "class GPTDataSetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \"\"\"\n",
    "        :param txt: source text\n",
    "        :param tokenizer: tokenizer object\n",
    "        :param max_length: number of token ids in a chunk\n",
    "        :param stride: shift across batches. For example, input/target pair batch 1 is [0:4]/[1:5], batch 2 is [1:5]/[2:6], etc.\n",
    "        \"\"\"\n",
    "        self.input_ids = []  # A list of tensors, or a multi-dimension array\n",
    "        self.target_ids = [] # A list of tensors, or a multi-dimension array\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)  # Tokenize the text and create token ids\n",
    "\n",
    "        # Create input/target pairs using a sliding window\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx) -> (Tensor, Tensor):\n",
    "        # Returns a pair of 1D tensors (input_ids, target_ids) at the index idx\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "ae9cd9f731cc7648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:38.898296Z",
     "start_time": "2024-10-24T16:26:38.896144Z"
    }
   },
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataSetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "6c7ddb305f72082c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:38.913753Z",
     "start_time": "2024-10-24T16:26:38.899105Z"
    }
   },
   "source": [
    "# batch_size: number of batches in input tensors or target tensors\n",
    "# max_length: number of token ids in a batch\n",
    "# stride: shift across batches in input tensors\n",
    "data_loader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(data_loader) # Iteration of the input/target tensor pairs\n",
    "print(next(data_iter))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464],\n",
      "        [1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899]])]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "d44d524dc645919e",
   "metadata": {},
   "source": [
    "# Creating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57493189dc8fe1",
   "metadata": {},
   "source": [
    "## Input Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a65c35fbc7bc0b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.068803Z",
     "start_time": "2024-10-24T16:26:38.916966Z"
    }
   },
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Creating a Dataloader\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=8, \n",
    "    max_length=max_length, \n",
    "    stride=max_length, \n",
    "    shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Input Token IDs (per batch): \\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "# Create an embedding layer\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(\"\\nEmbedding Layer:\\n\", token_embedding_layer)\n",
    "input_embeddings = token_embedding_layer(inputs)\n",
    "print(\"\\nInput Embeddings Shape:\\n\", input_embeddings.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Token IDs (per batch): \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "\n",
      "Embedding Layer:\n",
      " Embedding(50257, 256)\n",
      "\n",
      "Input Embeddings Shape:\n",
      " torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "b21c3d426cd5dcd8",
   "metadata": {},
   "source": [
    "## Input Positional Embeddings (Context Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb6213fd3d3dcfc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.071994Z",
     "start_time": "2024-10-24T16:26:39.069496Z"
    }
   },
   "source": [
    "context_length = max_length # Positional context has the same number of input tokens in a batch\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)  # We only need the same number of positional embeddings as the number of tokens in a batch\n",
    "\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))  # torch.arrange creates a tensor with values from 0 to context_length-1\n",
    "print(pos_embeddings.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "cbe0bcf88a54ea65",
   "metadata": {},
   "source": [
    "## Input Embeddings: Combine Input Token Embeddings and Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "eac408d75404cb4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.074721Z",
     "start_time": "2024-10-24T16:26:39.072598Z"
    }
   },
   "source": [
    "input_embeddings = input_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Self-Attention Mechanism",
   "id": "e9666d4f0c881397"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.077414Z",
     "start_time": "2024-10-24T16:26:39.075388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Say, we have a text input sequence of 6 word and each word/token has 3 dimension embedding.\n",
    "# So, the input sequence is of shape (6, 3)\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # word 1, Your (x^1)\n",
    "        [0.55, 0.87, 0.66], # word 2, journey (x^2)\n",
    "        [0.57, 0.85, 0.64], # word 3, starts (x^3)\n",
    "        [0.22, 0.58, 0.33], # word 4, with (x^4)\n",
    "        [0.77, 0.25, 0.10], # word 5, one (x^5)\n",
    "        [0.05, 0.80, 0.55], # word 6, step (x^6)\n",
    "    ]\n",
    ")"
   ],
   "id": "79139e2d0dfd4774",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simplified Self-Attention",
   "id": "7ca13e84cc0a6646"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Attention Scores",
   "id": "9e744b04147c176e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.084974Z",
     "start_time": "2024-10-24T16:26:39.078060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a matrix of Attention Scores\n",
    "attn_scores = torch.empty(inputs.shape[0], inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)  # dot product of each word/token with each other\n",
    "\n",
    "print(\"Attention scores matrix:\\n\", attn_scores)\n",
    "\n",
    "# Or, Calculate the Attention Score matrix by using matrix multiplication\n",
    "attn_scores = torch.mm(inputs, torch.t(inputs)) # Input matrix x Input matrix transpose\n",
    "\n",
    "print(\"Attention scores matrix:\\n\", attn_scores)\n",
    "\n",
    "# Or, a simplified presentation of matrix multiplication\n",
    "attn_scores = inputs @ inputs.T\n",
    "\n",
    "print(\"Attention scores matrix:\\n\", attn_scores)"
   ],
   "id": "c556d21dda3a88b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores matrix:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "Attention scores matrix:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "Attention scores matrix:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Attention Weights",
   "id": "1e32cd89232a04ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.088183Z",
     "start_time": "2024-10-24T16:26:39.085632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(\"Attention weights:\\n\", attn_weights)"
   ],
   "id": "edd03877aa678671",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      " tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Context Embeddings",
   "id": "36fb8a00f9dbb737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.091673Z",
     "start_time": "2024-10-24T16:26:39.088945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "## Calculate the Context Embedding\n",
    "context_embeddings = attn_weights @ inputs\n",
    "print(\"Context embeddings:\\n\", context_embeddings)"
   ],
   "id": "6d105320d85cedb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context embeddings:\n",
      " tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scaled Dot-Product Attention",
   "id": "6203cc5a8816a0ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Query, Key, and Value Matrices",
   "id": "458fa13be68103c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.095247Z",
     "start_time": "2024-10-24T16:26:39.092327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "d_in = inputs.shape[1]   # This is dimension of the input token, M in the NxM\n",
    "d_out = inputs.shape[1]\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ],
   "id": "3bbeeab6b0a2ab2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.099602Z",
     "start_time": "2024-10-24T16:26:39.096082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = inputs @ W_query\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"Query:\", query)\n",
    "print(\"Key:\", keys)\n",
    "print(\"Value:\", values)"
   ],
   "id": "ef7720f2e716fe37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: tensor([[0.3522, 0.3244, 0.4020],\n",
      "        [0.8520, 0.4161, 1.0138],\n",
      "        [0.8415, 0.4229, 0.9978],\n",
      "        [0.5096, 0.1904, 0.6187],\n",
      "        [0.4138, 0.4265, 0.4288],\n",
      "        [0.6408, 0.1414, 0.8070]])\n",
      "Key: tensor([[0.6813, 0.2706, 1.0793],\n",
      "        [0.7305, 0.4227, 1.1993],\n",
      "        [0.7355, 0.4227, 1.1901],\n",
      "        [0.3363, 0.2225, 0.6077],\n",
      "        [0.6184, 0.3038, 0.6909],\n",
      "        [0.3178, 0.2383, 0.7426]])\n",
      "Value: tensor([[0.4976, 0.9655, 0.7614],\n",
      "        [0.9074, 1.3518, 1.5075],\n",
      "        [0.8976, 1.3391, 1.4994],\n",
      "        [0.5187, 0.7319, 0.8493],\n",
      "        [0.4699, 0.7336, 0.9307],\n",
      "        [0.6446, 0.9045, 0.9814]])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Attention Scores",
   "id": "661d583cf291800f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.103007Z",
     "start_time": "2024-10-24T16:26:39.100378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_scores = query @ keys.T\n",
    "print(\"Attention Score: \", attn_scores)"
   ],
   "id": "3e56d39eb2ef8272",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Score:  tensor([[0.7616, 0.8765, 0.8746, 0.4349, 0.5941, 0.4877],\n",
      "        [1.7872, 2.0141, 2.0091, 0.9952, 1.3538, 1.1227],\n",
      "        [1.7646, 1.9901, 1.9852, 0.9834, 1.3383, 1.1091],\n",
      "        [1.0664, 1.1947, 1.1916, 0.5897, 0.8004, 0.6667],\n",
      "        [0.8601, 0.9968, 0.9950, 0.4947, 0.6817, 0.5516],\n",
      "        [1.3458, 1.4957, 1.4915, 0.7374, 0.9968, 0.8366]])\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Attention Weights",
   "id": "d86d5870ad49b94c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.106823Z",
     "start_time": "2024-10-24T16:26:39.103888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "print(\"Attention weights:\", attn_weights)"
   ],
   "id": "4a1c829d441e7f26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([[0.1747, 0.1866, 0.1864, 0.1446, 0.1586, 0.1491],\n",
      "        [0.1862, 0.2123, 0.2117, 0.1179, 0.1450, 0.1269],\n",
      "        [0.1859, 0.2118, 0.2112, 0.1184, 0.1454, 0.1273],\n",
      "        [0.1798, 0.1936, 0.1932, 0.1365, 0.1542, 0.1427],\n",
      "        [0.1751, 0.1895, 0.1893, 0.1418, 0.1579, 0.1465],\n",
      "        [0.1837, 0.2003, 0.1998, 0.1293, 0.1501, 0.1369]])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Context Embeddings",
   "id": "efecafb33982f6ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.109814Z",
     "start_time": "2024-10-24T16:26:39.107598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_embeddings = attn_weights @ values\n",
    "print(\"Context embedding:\", context_embeddings)"
   ],
   "id": "f5e902501d2fccc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context embedding: tensor([[0.6692, 1.0276, 1.1106],\n",
      "        [0.6864, 1.0577, 1.1389],\n",
      "        [0.6860, 1.0570, 1.1383],\n",
      "        [0.6738, 1.0361, 1.1180],\n",
      "        [0.6711, 1.0307, 1.1139],\n",
      "        [0.6783, 1.0441, 1.1252]])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Implementing a Scaled Dot-Product Attention Class",
   "id": "1b584f558bfdf89b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.112936Z",
     "start_time": "2024-10-24T16:26:39.110431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use nn.Parameter to create weight matrices\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(SelfAttention_v1, self).__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        query = inputs @ self.W_query\n",
    "        key = inputs @ self.W_key\n",
    "        value = inputs @ self.W_value\n",
    "\n",
    "        attn_scores = query @ key.T\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_embedding = attn_weights @ value\n",
    "        return context_embedding"
   ],
   "id": "ce8fc8f0864889be",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.116573Z",
     "start_time": "2024-10-24T16:26:39.113527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the SelfAttention_v1 class\n",
    "torch.manual_seed(123)\n",
    "\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "self_attn_v1 = SelfAttention_v1(d_in, d_out)\n",
    "context_embedding_v1 = self_attn_v1(inputs)\n",
    "print(\"Context embeddings:\\n\", context_embedding_v1)"
   ],
   "id": "7fd2975eb341ff32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context embeddings:\n",
      " tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.120249Z",
     "start_time": "2024-10-24T16:26:39.117408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use nn.Linear to create weight matrices\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.attn_weights = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        query = self.W_query(inputs)\n",
    "        key = self.W_key(inputs)\n",
    "        value = self.W_value(inputs)\n",
    "\n",
    "        attn_scores = query @ key.T\n",
    "\n",
    "        self.attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_embedding = attn_weights @ value\n",
    "        return context_embedding"
   ],
   "id": "80318d005f6274e0",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.124943Z",
     "start_time": "2024-10-24T16:26:39.120901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the SelfAttention_v2 class\n",
    "torch.manual_seed(789)\n",
    "\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "self_attn_v2 = SelfAttention_v2(d_in, d_out)\n",
    "context_embedding_v2 = self_attn_v2(inputs)\n",
    "print(\"Context embeddings:\\n\", context_embedding_v2)"
   ],
   "id": "c5586fbf6fb4d217",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context embeddings:\n",
      " tensor([[-0.0776,  0.0699],\n",
      "        [-0.0789,  0.0732],\n",
      "        [-0.0789,  0.0732],\n",
      "        [-0.0781,  0.0707],\n",
      "        [-0.0775,  0.0706],\n",
      "        [-0.0785,  0.0714]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Causal Attention",
   "id": "882aaf7abc8715d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Getting attention scores by using the same steps as the scaled dot-product attention",
   "id": "fac8fb7753d37a61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.128514Z",
     "start_time": "2024-10-24T16:26:39.125585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "self_attn_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "queries = self_attn_v2.W_query(inputs)\n",
    "keys = self_attn_v2.W_key(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "print(attn_scores)"
   ],
   "id": "97180db77c39d860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2118, 0.1588, 0.1574, 0.0699, 0.0885, 0.0899],\n",
      "        [0.2676, 0.2249, 0.2226, 0.1051, 0.1195, 0.1361],\n",
      "        [0.2622, 0.2215, 0.2193, 0.1038, 0.1175, 0.1344],\n",
      "        [0.1496, 0.1257, 0.1244, 0.0587, 0.0668, 0.0760],\n",
      "        [0.0926, 0.0984, 0.0972, 0.0506, 0.0479, 0.0662],\n",
      "        [0.2108, 0.1664, 0.1649, 0.0754, 0.0907, 0.0973]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Masking Attention Score",
   "id": "7b88a6697d88a39b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.132070Z",
     "start_time": "2024-10-24T16:26:39.129233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "length = attn_weights.shape[0]\n",
    "\n",
    "mask = torch.triu(torch.ones(length, length), diagonal=1)\n",
    "masked_attn_score = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked_attn_score)"
   ],
   "id": "34baa78cd06bf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2118,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.2676, 0.2249,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.2622, 0.2215, 0.2193,   -inf,   -inf,   -inf],\n",
      "        [0.1496, 0.1257, 0.1244, 0.0587,   -inf,   -inf],\n",
      "        [0.0926, 0.0984, 0.0972, 0.0506, 0.0479,   -inf],\n",
      "        [0.2108, 0.1664, 0.1649, 0.0754, 0.0907, 0.0973]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Normalizing to Attention Weights",
   "id": "431733d13a71e1aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.139178Z",
     "start_time": "2024-10-24T16:26:39.136712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights = torch.softmax(masked_attn_score / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ],
   "id": "4460f6c7e41ac3cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5075, 0.4925, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3399, 0.3303, 0.3298, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2562, 0.2519, 0.2517, 0.2402, 0.0000, 0.0000],\n",
      "        [0.2021, 0.2030, 0.2028, 0.1962, 0.1959, 0.0000],\n",
      "        [0.1758, 0.1704, 0.1702, 0.1598, 0.1615, 0.1623]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Add Dropouts",
   "id": "769056ae3480c0bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.143025Z",
     "start_time": "2024-10-24T16:26:39.139965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "dropped_out_attn_weights = dropout(attn_weights)\n",
    "print(dropped_out_attn_weights)"
   ],
   "id": "c1186912a755e78d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6799, 0.6606, 0.6595, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5038, 0.5033, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4060, 0.0000, 0.3924, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3408, 0.3404, 0.3196, 0.3230, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Implementing a Causal Attention Class",
   "id": "b8312012aa15cbc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.146820Z",
     "start_time": "2024-10-24T16:26:39.143745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.d_in = d_in\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is a 3-dimensional tensor\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "        # Mask attention score\n",
    "        attn_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.drop_out(attn_weights)\n",
    "        \n",
    "        context_embedding = attn_weights @ values\n",
    "        \n",
    "        return context_embedding"
   ],
   "id": "d8b6935f52b4867",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.151003Z",
     "start_time": "2024-10-24T16:26:39.147607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0) # Create a batch of input tokens (2 x (6 x 3))\n",
    "print(batch.shape)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs.shape)"
   ],
   "id": "c2977658076a6dbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multi-head Attention",
   "id": "33a9fe431cb24e6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.156509Z",
     "start_time": "2024-10-24T16:26:39.151650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        \"\"\"\n",
    "        Class of Multi-head Attention\n",
    "        :param d_in: dimension of input embedding\n",
    "        :param d_out: dimension of context embedding\n",
    "        :param context_length: dimension of attention weights, which is the number of inputs, d_in\n",
    "        :param dropout: \n",
    "        :param num_heads: \n",
    "        :param qkv_bias: \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # dimension per head\n",
    "        \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # the final concatenation of context embedding\n",
    "\n",
    "        # Define masking\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [n, [N, M]] n batches of input sequence (N x M)\n",
    "        \"\"\"\n",
    "\n",
    "        b, num_tokens, d_in = x.shape  # b: the number of batches\n",
    "        \n",
    "        print(f\"input shape: {x.shape}\")\n",
    "        \n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        print(f\"keys shape: {keys.shape}\")\n",
    "        \n",
    "        # Splits query, key, value matrices in\n",
    "        # to several parts by d_out\n",
    "        # self.num_heads x self.head_dim = d_out\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(\n",
    "            b, num_tokens, self.num_heads, self.head_dim\n",
    "        )\n",
    "        print(f\"keys shape after split: {keys.shape}\")\n",
    "        \n",
    "        # Transposes from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        print(f\"keys shape after transpose(1,2): {keys.shape}\")\n",
    "\n",
    "        # (b, num_heads, head_dim, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        print(f\"attention score shape: {attn_scores.shape}\")\n",
    "\n",
    "        # Causal masking\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        \n",
    "        # Dropout attn weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine views\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        print(f\"context vec shape: {context_vec.shape}\")\n",
    "        \n",
    "        return context_vec"
   ],
   "id": "d6298400288e52e",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.161205Z",
     "start_time": "2024-10-24T16:26:39.157267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ],
   "id": "4d6af5758da5a07f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 6, 3])\n",
      "keys shape: torch.Size([2, 6, 2])\n",
      "keys shape after split: torch.Size([2, 6, 2, 1])\n",
      "keys shape after transpose(1,2): torch.Size([2, 2, 6, 1])\n",
      "attention score shape: torch.Size([2, 2, 6, 6])\n",
      "context vec shape: torch.Size([2, 6, 2])\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:59:05.838766Z",
     "start_time": "2024-10-24T16:59:05.830657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4).\n",
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "print(a.transpose(1,2))\n",
    "\n",
    "print(a.transpose(1,2).transpose(2, 3))\n",
    "\n",
    "print(a @ a.transpose(2, 3))\n",
    "\n",
    "print(a.view(1,2,12))"
   ],
   "id": "c3074f4aa4d00670",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
      "          [0.0772, 0.3565, 0.1479, 0.5331]],\n",
      "\n",
      "         [[0.8993, 0.0390, 0.9268, 0.7388],\n",
      "          [0.4066, 0.2318, 0.4545, 0.9737]],\n",
      "\n",
      "         [[0.7179, 0.7058, 0.9156, 0.4340],\n",
      "          [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
      "tensor([[[[0.2745, 0.0772],\n",
      "          [0.6584, 0.3565],\n",
      "          [0.2775, 0.1479],\n",
      "          [0.8573, 0.5331]],\n",
      "\n",
      "         [[0.8993, 0.4066],\n",
      "          [0.0390, 0.2318],\n",
      "          [0.9268, 0.4545],\n",
      "          [0.7388, 0.9737]],\n",
      "\n",
      "         [[0.7179, 0.4606],\n",
      "          [0.7058, 0.5159],\n",
      "          [0.9156, 0.4220],\n",
      "          [0.4340, 0.5786]]]])\n",
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n",
      "tensor([[[0.2745, 0.6584, 0.2775, 0.8573, 0.8993, 0.0390, 0.9268, 0.7388,\n",
      "          0.7179, 0.7058, 0.9156, 0.4340],\n",
      "         [0.0772, 0.3565, 0.1479, 0.5331, 0.4066, 0.2318, 0.4545, 0.9737,\n",
      "          0.4606, 0.5159, 0.4220, 0.5786]]])\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:26:39.167581Z",
     "start_time": "2024-10-24T16:26:39.166243Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c09250a9650ab9d1",
   "outputs": [],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
