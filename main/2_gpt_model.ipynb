{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create a LLM (GPT-2) Model",
   "id": "279c694639ae6ad8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T23:06:30.254375Z",
     "start_time": "2024-10-27T23:06:30.252362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model configuration\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length, the number of tokens\n",
    "    \"emb_dim\": 768,          # Embedding dimension, for both tokens and contexts\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias, whether to include a bias vector in the Linear layers for query, key, and value computations.\n",
    "}"
   ],
   "id": "92f1e5561e2a78a0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## A Dummy GPT Model Class",
   "id": "1f0c49202562df6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T23:06:32.228203Z",
     "start_time": "2024-10-27T23:06:30.255479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        # create embeddings from input tokens\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        # Apply final layer normalization after multi-head attention\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # Logit refers to the unscaled output of a model, often used in machine learning before applying a softmax function to convert them into probabilities; logits are crucial for calculating loss functions like cross-entropy, which measures the difference between predicted and true distributions\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, emd_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps # a constant to prevent division by zero\n",
    "        # scale and shift parameters, which are learned during training, they have the same dimensions as the input tensor\n",
    "        self.scale = nn.Parameter(torch.ones(emd_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emd_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ],
   "id": "de120c295db13054",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T23:06:32.449332Z",
     "start_time": "2024-10-27T23:06:32.229411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenization\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch = []\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "print(batch)"
   ],
   "id": "b9ba31b1be01cafa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T23:06:33.020360Z",
     "start_time": "2024-10-27T23:06:32.449939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "\n",
    "print(\"Logits output shape:\", logits.shape)\n",
    "print(logits)"
   ],
   "id": "35c14fdd37d44bc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Layer Normalization",
   "id": "9d356c10734dacff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T23:14:04.576101Z",
     "start_time": "2024-10-27T23:14:04.569413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "input = torch.randn(2, 5)\n",
    "print(\"Input: \", input)\n",
    "\n",
    "# Create a layer that takes 5 activations and outputs 6 activations\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "print(\"Layer: \", layer)\n",
    "\n",
    "output = layer(input)\n",
    "print(\"Output: \", output)\n",
    "\n",
    "# Calculate mean and variance\n",
    "mean = output.mean(dim=-1, keepdim=True)\n",
    "variance = output.var(dim=-1, keepdim=True)\n",
    "print(\"Mean before normalization: \\n\", mean)\n",
    "print(\"Variance before normalization: \\n\", variance)\n",
    "\n",
    "# Apply layer normalization\n",
    "layer_norm = (output - mean) / torch.sqrt(variance)\n",
    "print(\"Layer normalized: \\n\", layer_norm)\n",
    "mean = layer_norm.mean(dim=-1, keepdim=True)\n",
    "variance = layer_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Mean after normalization: \\n\", mean)\n",
    "print(\"Variance after normalization: \\n\", variance)"
   ],
   "id": "4022cbbd69acc136",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n",
      "Layer:  Sequential(\n",
      "  (0): Linear(in_features=5, out_features=6, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "Output:  tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean before normalization: \n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance before normalization: \n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n",
      "Layer normalized: \n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean after normalization: \n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance after normalization: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GELU Activation Function",
   "id": "bdb3fec584fd57de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T23:36:03.573559Z",
     "start_time": "2024-10-27T23:36:03.564507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ],
   "id": "3376737792a1365a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feed Forward Network",
   "id": "cf2271244aa445d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T23:36:14.461849Z",
     "start_time": "2024-10-27T23:36:14.458583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "id": "f040de470ebc8dfd",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T23:37:19.222256Z",
     "start_time": "2024-10-27T23:37:19.184623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ],
   "id": "da92535a5ff4109b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Shortcut Connection",
   "id": "9a3ad44927c5d626"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T02:46:37.418384Z",
     "start_time": "2024-10-28T02:46:37.411722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
    "                          GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
    "                          GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]),\n",
    "                          GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]),\n",
    "                          GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]),\n",
    "                          GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ],
   "id": "d924890bef4ceefe",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T03:28:09.730722Z",
     "start_time": "2024-10-28T03:28:09.724867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)"
   ],
   "id": "817454607a51e96",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T03:28:11.437343Z",
     "start_time": "2024-10-28T03:28:11.428929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "print(\"Model without shortcut:\")\n",
    "print_gradients(model_without_shortcut, sample_input)\n",
    "print(\"\\nModel with shortcut:\")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ],
   "id": "5dc0e62057109058",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model without shortcut:\n",
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152039906941354\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n",
      "\n",
      "Model with shortcut:\n",
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## A Transformer Block",
   "id": "7710549001a7847b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T03:41:48.784768Z",
     "start_time": "2024-10-28T03:41:48.776250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ],
   "id": "f9445d63df1ac24c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T03:57:12.069947Z",
     "start_time": "2024-10-28T03:57:12.064087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Attention module\n",
    "        self.att = MultiHeadAttention(d_in=cfg[\"emb_dim\"], \n",
    "                                      d_out=cfg[\"emb_dim\"],\n",
    "                                      context_length=cfg[\"context_length\"],\n",
    "                                      num_heads=cfg[\"n_heads\"],\n",
    "                                      dropout=cfg[\"drop_rate\"],\n",
    "                                      qkv_bias=cfg[\"qkv_bias\"])\n",
    "        # Feed Forward module, which expand the network to 4 times bigger.\n",
    "        self.ff = FeedForward(cfg)\n",
    "        # Layer Norms modules\n",
    "        self.norm1 = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        # Dropout modules\n",
    "        self.dropout_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        # Apply layer normalization before multi-head attention\n",
    "        x = self.norm1(x)\n",
    "        # Apply multi-head attention\n",
    "        x = self.att(x)\n",
    "        # Apply dropout after attention to prevent overfitting\n",
    "        x = self.dropout_shortcut(x)\n",
    "        # Add the shortcut connection after attention\n",
    "        x = x + shortcut\n",
    "    \n",
    "        shortcut = x\n",
    "        # Apply layer normalization after multi-head attention\n",
    "        x = self.norm2(x)\n",
    "        # Apply feed forward network after attention\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ],
   "id": "7374bd2acc6b6d32",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T03:57:13.350607Z",
     "start_time": "2024-10-28T03:57:13.271151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ],
   "id": "811c559f8788e7f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Assembling Transformer Blocks into a GPT Model",
   "id": "1cac5922600327a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T04:41:12.903873Z",
     "start_time": "2024-10-28T04:41:12.896297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Create input token embeddings\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]) \n",
    "        # Create positional embeddings\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # Dropout layer\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Added transformer block\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        # Added final output layer\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        \"\"\"\n",
    "        :param in_idx: Tokenized input tensor\n",
    "        :return: output embeddings\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        token_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        \n",
    "        x = token_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits"
   ],
   "id": "848edbd4c189d1b1",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T04:47:22.936502Z",
     "start_time": "2024-10-28T04:47:21.868098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input token batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal number of parameters: {total_params:,}\")\n",
    "\n",
    "print(\"\\nToken embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "\n",
    "total_params_gpt2 = (\n",
    "        total_params - sum(p.numel()\n",
    "                           for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"\\nNumber of trainable parameters \"\n",
    "      f\"considering weight tying: {total_params_gpt2:,}\"\n",
    "      )\n",
    "\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"\\nTotal size of the model: {total_size_mb:.2f} MB\")\n"
   ],
   "id": "7b0631ca1adb24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.1050, -0.0054, -0.9680,  ...,  3.0550,  2.3504, -2.9953],\n",
      "         [-3.4404, -2.7845, -2.6117,  ...,  0.8947,  4.6135, -2.8290],\n",
      "         [-0.1715, -0.8735,  2.6575,  ...,  0.9795,  1.5972, -3.4830],\n",
      "         [-2.6278, -0.3963,  0.2422,  ...,  0.9697,  2.0820,  0.3534]],\n",
      "\n",
      "        [[-2.8959,  2.0748, -0.4365,  ..., -0.4643,  2.6097, -1.5061],\n",
      "         [-0.1044,  0.9152, -1.6846,  ...,  0.7598,  3.3212, -0.5796],\n",
      "         [-0.3383,  2.1516, -1.7438,  ...,  2.1214,  2.1429, -0.7223],\n",
      "         [-3.3636,  2.2360,  0.6698,  ...,  1.9665,  1.5848, -0.4326]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Total number of parameters: 162,971,136\n",
      "\n",
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n",
      "\n",
      "Number of trainable parameters considering weight tying: 124,373,760\n",
      "\n",
      "Total size of the model: 621.69 MB\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generating Texts",
   "id": "4388b2953ea60a18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T21:56:11.633481Z",
     "start_time": "2024-10-28T21:56:11.627722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \"\"\"  \n",
    "    :param model: GPT model    :param idx: input    :param max_new_tokens: how many tokens to generate    :param context_size: size of tokens to consider for the context    :return:   \n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crops the current context to fit the model’s maximum context size  \n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # Generate the output logits  \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        # Sample the token with the highest probability  \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        # Append the new token to the input  \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ],
   "id": "9c896e9a48580250",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T21:54:18.028463Z",
     "start_time": "2024-10-28T21:54:18.024854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "# Tokenize the input (1 dimensional tensor)\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "# Convert the encoded tensor to a 2D tensor (a matrix)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ],
   "id": "b348b213aa902715",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T21:56:15.629940Z",
     "start_time": "2024-10-28T21:56:15.008931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ],
   "id": "6181a6fa4355f4ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 34359,  2312, 48810, 36468,  8267, 36678]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T21:56:17.580424Z",
     "start_time": "2024-10-28T21:56:17.576107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ],
   "id": "baeb4f241d648d76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am RG Theseknife CalderILL 268\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b4b3e90bcebf7237"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
